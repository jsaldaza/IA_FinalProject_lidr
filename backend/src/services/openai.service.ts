/* eslint-disable @typescript-eslint/no-explicit-any, @typescript-eslint/no-non-null-assertion */
import OpenAI from 'openai';
import { config } from '../config';
import { configService } from './config.service';
import { ChatCompletionMessageParam } from 'openai/resources/chat';
import { AIMetricsService } from './ai-metrics.service';
import { AnalysisCacheService } from './analysis-cache.service';
import { TokenCostControlMiddleware } from '../middleware/token-cost-control.middleware';
import { StructuredLogger } from '../utils/structured-logger';
import { circuitBreakers, CircuitBreakerOpenError } from '../utils/circuit-breaker';
import { aiCostPredictor, CostEstimate } from '../utils/ai-cost-predictor';

// Tipos para nuestro servicio
export interface AIResponse {
    success: boolean;
    data?: string;
    error?: string;
    usage?: {
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
    };
    costInfo?: {
        estimatedCost: number;
        actualCost: number;
        model: string;
        savings?: number;
    };
    circuitBreakerUsed?: boolean;
}

export interface ConversationContext {
    messages: ChatCompletionMessageParam[];
    analysisId?: string;
    projectId?: string;
}

export interface AIServiceConfig {
    temperature?: number;
    maxTokens?: number;
    model?: string;
}

// Prompts centralizados y OPTIMIZADOS para m√°ximo ahorro
export const PROMPTS = {
    ANALYSIS: `Eres QA experto. Analiza el requisito:
1. Resumen claro
2. Riesgos principales  
3. Casos edge
4. Seguridad cr√≠tica

Requisito:`,

    QUESTIONS: `Basado en el an√°lisis, genera 3-5 preguntas QA clave:
- Clarificaciones necesarias
- √Åreas de riesgo
- Casos l√≠mite

An√°lisis:`,

    STRATEGY: `Crea estrategia de pruebas:
1. Enfoque general
2. Escenarios clave
3. Cobertura
4. Prioridades

An√°lisis:`,

    GENERATE_QUESTIONS: `Experto QA: Del resumen, genera exactamente 3 preguntas cr√≠ticas en JSON como array de strings.

Resumen:`,

    CONVERSATIONAL_ANALYSIS: `Analista QA senior. Analiza requisitos interactivamente con preguntas espec√≠ficas para testing.`,
} as const;

// Utilidades para optimizaci√≥n de tokens
export class TokenOptimizer {
    static optimizePrompt(prompt: string): string {
        return prompt
            .replace(/\s+/g, ' ')           // M√∫ltiples espacios a uno solo
            .replace(/\n\s*\n/g, '\n')      // M√∫ltiples saltos de l√≠nea a uno solo
            .trim();
    }

    static truncateContext(text: string, maxTokens: number = 1500): string {
        // Estimaci√≥n simple: ~4 caracteres por token
        const maxChars = maxTokens * 4;
        if (text.length <= maxChars) return text;
        
        return text.substring(0, maxChars) + '...';
    }

    static calculateEstimatedTokens(text: string): number {
        // Estimaci√≥n aproximada: ~4 caracteres por token
        return Math.ceil(text.length / 4);
    }

    static shouldUseGPT4(complexity: 'low' | 'medium' | 'high', estimatedTokens: number): boolean {
        // Usar GPT-4 solo para casos complejos o cuando sea realmente necesario
        if (complexity === 'high') return true;
        if (complexity === 'medium' && estimatedTokens > 1000) return true;
        return false;
    }
}

const openai = new OpenAI({
    apiKey: configService.getOpenAIConfig().apiKey,
});

// Exportar tambi√©n la instancia de openai para uso directo
export { openai };

export class OpenAIService {
    private defaultConfig: AIServiceConfig;
    private conversations: Map<string, ConversationContext>;

    constructor() {
        // Validar que la API key est√© configurada
        const openaiConfig = configService.getOpenAIConfig();
        if (!openaiConfig.apiKey) {
            throw new Error('‚ùå OPENAI_API_KEY no est√° configurada. Revisa tu archivo .env');
        }

        this.defaultConfig = {
            temperature: openaiConfig.temperature,
            maxTokens: openaiConfig.maxTokens,
            model: openaiConfig.model
        };
        this.conversations = new Map();
        
        StructuredLogger.info('OpenAI Service initialized with Circuit Breaker', { method: 'constructor' });
    }

    /**
     * Predict cost before making AI call
     */
    async predictRequestCost(
        input: string, 
        complexity: 'low' | 'medium' | 'high' = 'medium',
        userId?: string
    ): Promise<CostEstimate> {
        return await aiCostPredictor.predictCost(input, complexity, userId);
    }

    /**
     * Check if request is within budget before execution
     */
    async validateBudget(userId: string, estimatedCost: number): Promise<boolean> {
        const budgetCheck = await aiCostPredictor.checkBudget(userId, estimatedCost);
        
        if (!budgetCheck.withinBudget) {
            StructuredLogger.warn('Budget exceeded for AI request', {
                method: 'validateBudget',
                userId,
                warnings: budgetCheck.warnings
            });
        }
        
        return budgetCheck.withinBudget;
    }

    private getConversationKey(analysisId?: string, projectId?: string): string {
        return `${analysisId || 'default'}-${projectId || 'default'}`;
    }

    private optimizeConfigForTask(task: 'analysis' | 'questions' | 'strategy' | 'chat'): AIServiceConfig {
        const configs = {
            analysis: { temperature: 0.2, maxTokens: 1500 },
            questions: { temperature: 0.4, maxTokens: 1000 },
            strategy: { temperature: 0.3, maxTokens: 2000 },
            chat: { temperature: 0.7, maxTokens: 1000 }
        };

        return { ...this.defaultConfig, ...configs[task] };
    }

    public getConversationContext(analysisId?: string, projectId?: string): ConversationContext {
        const key = this.getConversationKey(analysisId, projectId);
        if (!this.conversations.has(key)) {
            this.conversations.set(key, {
                messages: [],
                analysisId,
                projectId
            });
        }
        return this.conversations.get(key)!;
    }

    public async chat(
        prompt: string,
        systemPrompt?: string,
        config?: AIServiceConfig,
        analysisId?: string,
        projectId?: string
    ): Promise<AIResponse> {
        try {
            const context = this.getConversationContext(analysisId, projectId);

            // Agregar el system prompt si es nuevo o diferente al √∫ltimo
            if (systemPrompt && (context.messages.length === 0 ||
                (context.messages[0].role === 'system' && context.messages[0].content !== systemPrompt))) {
                context.messages.unshift({ role: 'system', content: systemPrompt });
            }

            // Agregar el nuevo mensaje del usuario
            context.messages.push({ role: 'user', content: prompt });

            const response = await openai.chat.completions.create({
                model: config?.model || this.defaultConfig.model!,
                messages: context.messages,
                temperature: config?.temperature || this.defaultConfig.temperature!,
                max_tokens: config?.maxTokens || this.defaultConfig.maxTokens!
            });

            const assistantMessage = response.choices[0].message;

            // Guardar la respuesta en el contexto
            if (assistantMessage) {
                context.messages.push({
                    role: assistantMessage.role,
                    content: assistantMessage.content || ''
                });
            }

            return {
                success: true,
                data: assistantMessage?.content || '',
                usage: {
                    promptTokens: response.usage?.prompt_tokens || 0,
                    completionTokens: response.usage?.completion_tokens || 0,
                    totalTokens: response.usage?.total_tokens || 0
                }
            };
        } catch (error: any) {
            StructuredLogger.error('OpenAI Service Error', error as Error, { method: 'chat' });
            return {
                success: false,
                error: error.message || 'Failed to get AI response'
            };
        }
    }

    public clearConversation(analysisId?: string, projectId?: string): void {
        const key = this.getConversationKey(analysisId, projectId);
        this.conversations.delete(key);
    }

    public async analyzeWithCriticalThinking(
        requirement: string,
        analysisId?: string,
        projectId?: string
    ): Promise<AIResponse> {
    StructuredLogger.info('Starting analyzeWithCriticalThinking', { method: 'analyzeWithCriticalThinking' });

        const systemPrompt = `You are an expert QA analyst and critical thinker with over 20 years of experience in software testing. 
Your task is to:
1. Thoroughly analyze the given requirement
2. Identify potential ambiguities, risks, and edge cases
3. Think critically about business and technical implications
4. Consider security, performance, and user experience aspects
5. Question assumptions and identify missing information

Be thorough, critical, and detailed in your analysis. Ask probing questions that challenge assumptions and reveal potential issues.`;

    StructuredLogger.debug('System prompt prepared', { method: 'analyzeWithCriticalThinking' });

        try {
            const response = await this.chat(
                requirement,
                systemPrompt,
                { temperature: 0.7, model: 'gpt-3.5-turbo' },
                analysisId,
                projectId
            );
            StructuredLogger.ai('analyzeWithCriticalThinking response', { method: 'analyzeWithCriticalThinking' });
            return response;
        } catch (error) {
            StructuredLogger.error('Error in analyzeWithCriticalThinking', error as Error, { method: 'analyzeWithCriticalThinking' });
            return {
                success: false,
                error: error instanceof Error ? error.message : 'Unknown error occurred'
            };
        }
    }

    public async generateTestStrategy(
        requirement: string,
        analysis: string,
        questions: string[],
        analysisId?: string,
        projectId?: string
    ): Promise<AIResponse> {
        const context = `
Requirement: ${requirement}

Analysis: ${analysis}

Key Questions Identified:
${questions.map((q, i) => `${i + 1}. ${q}`).join('\n')}
`;

        const systemPrompt = `You are an expert QA strategist with over 20 years of experience in software testing.
Your task is to create a comprehensive test strategy that:
1. Addresses all identified questions and concerns
2. Covers all aspects of testing (functional, non-functional, security, performance)
3. Prioritizes test areas based on risk and business impact
4. Suggests specific test approaches and techniques
5. Identifies required tools and resources
6. Considers automation opportunities

Be specific, practical, and thorough in your strategy. Think about both immediate testing needs and long-term quality assurance.`;

        return this.chat(
            context,
            systemPrompt,
            { temperature: 0.7, model: 'gpt-4' },
            analysisId,
            projectId
        );
    }

    public async generateTestScenarios(
        strategy: string,
        analysisId?: string,
        projectId?: string
    ): Promise<AIResponse> {
        const systemPrompt = `You are an expert test designer with deep experience in creating effective test scenarios.
Your task is to:
1. Create detailed test scenarios based on the provided strategy
2. Cover both happy paths and edge cases
3. Include preconditions and expected results
4. Consider different user roles and permissions
5. Include data variations and boundary conditions

Format each scenario clearly with:
- Scenario ID and title
- Preconditions
- Steps
- Expected results
- Priority level`;

        return this.chat(
            strategy,
            systemPrompt,
            { temperature: 0.7, model: 'gpt-3.5-turbo' },
            analysisId,
            projectId
        );
    }

    static async refineAnalysis(requirement: string, questionsAndAnswers: Array<{ question: string; answer: string }>) {
        try {
            const prompt = `
                Como experto en testing y an√°lisis de requerimientos, por favor refina el siguiente an√°lisis bas√°ndote en las respuestas proporcionadas.

                REQUERIMIENTO:
                ${requirement}

                PREGUNTAS Y RESPUESTAS:
                ${questionsAndAnswers.map(qa => `
                    Pregunta: ${qa.question}
                    Respuesta: ${qa.answer}
                `).join('\n')}

                Por favor, proporciona:
                1. Un resumen actualizado del an√°lisis
                2. Lista de red flags actualizada
                3. Estrategias de prueba actualizadas

                Formato de respuesta (JSON):
                {
                    "summary": "Resumen actualizado del an√°lisis...",
                    "redFlags": ["Red flag 1", "Red flag 2", ...],
                    "testStrategies": ["Estrategia 1", "Estrategia 2", ...]
                }
            `;

            const completion = await openai.chat.completions.create({
                model: config.openai.model,
                messages: [
                    {
                        role: "system",
                        content: "Eres un experto en testing y an√°lisis de requerimientos. Tu tarea es refinar el an√°lisis bas√°ndote en las respuestas proporcionadas."
                    },
                    {
                        role: "user",
                        content: prompt
                    }
                ],
                temperature: 0.7,
                max_tokens: config.openai.maxTokens,
                response_format: { type: "json_object" }
            });

            const response = completion.choices[0].message.content;
            if (!response) {
                throw new Error('No response from OpenAI');
            }

            return JSON.parse(response);
        } catch (error) {
            StructuredLogger.error('OpenAI Service Error', error as Error, { method: 'refineAnalysis' } as any);
            throw error;
        }
    }

    /**
     * Calculate actual cost of AI request
     */
    private calculateActualCost(inputTokens: number, outputTokens: number, model: string): number {
        // Simplified cost calculation - should match AICostPredictor pricing
        const pricing: Record<string, { input: number; output: number }> = {
            'gpt-4': { input: 0.03, output: 0.06 },
            'gpt-4-turbo': { input: 0.01, output: 0.03 },
            'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 }
        };

        const modelPricing = pricing[model] || pricing['gpt-3.5-turbo'];
        const inputCost = (inputTokens / 1000) * modelPricing.input;
        const outputCost = (outputTokens / 1000) * modelPricing.output;
        
        return inputCost + outputCost;
    }

    // M√©todos centralizados para funcionalidades principales
    async analyzeRequirement(requirement: string, userId?: string): Promise<AIResponse> {
        try {
            // üéØ STEP 1: Verificar cach√© primero
            const cachedAnalysis = await AnalysisCacheService.getCachedAnalysis(requirement);
            if (cachedAnalysis) {
                StructuredLogger.info('Using cached analysis', { method: 'analyzeRequirement' });
                return {
                    success: true,
                    data: cachedAnalysis.content,
                    usage: {
                        promptTokens: 0,
                        completionTokens: 0,
                        totalTokens: 0
                    }
                };
            }

            // üéØ STEP 2: Predicci√≥n de costos
            const costEstimate = await this.predictRequestCost(requirement, 'medium', userId);
            StructuredLogger.info('Cost prediction', {
                method: 'analyzeRequirement',
                estimatedCost: costEstimate.estimatedCost,
                model: costEstimate.model
            });

            // üéØ STEP 3: Validar presupuesto si hay userId
            if (userId) {
                const budgetOk = await this.validateBudget(userId, costEstimate.estimatedCost);
                if (!budgetOk) {
                    return {
                        success: false,
                        error: 'Request would exceed budget limits. Consider using cache or simpler analysis.',
                        costInfo: {
                            estimatedCost: costEstimate.estimatedCost,
                            actualCost: 0,
                            model: costEstimate.model
                        }
                    };
                }
            }

            // üéØ STEP 4: Optimizar prompts y texto
            const optimizedPrompt = TokenOptimizer.optimizePrompt(PROMPTS.ANALYSIS);
            const optimizedRequirement = TokenOptimizer.truncateContext(requirement, 1200);
            
            // üéØ STEP 5: Usar modelo recomendado por el cost predictor
            const optimalModel = costEstimate.model;
            const taskConfig = this.optimizeConfigForTask('analysis');

            // üéØ STEP 6: Ejecutar con Circuit Breaker
            const completion = await circuitBreakers.openai.execute(async () => {
                return await openai.chat.completions.create({
                    model: optimalModel,
                    messages: [
                        { role: "system", content: optimizedPrompt },
                        { role: "user", content: optimizedRequirement }
                    ],
                    temperature: taskConfig.temperature,
                    max_tokens: Math.min(taskConfig.maxTokens || 1500, 1000),
                });
            });

            const content = completion.choices[0]?.message?.content;
            if (!content) {
                throw new Error('No response from OpenAI');
            }

            // üéØ STEP 7: Calcular costo real
            const actualCost = this.calculateActualCost(
                completion.usage?.prompt_tokens || 0,
                completion.usage?.completion_tokens || 0,
                optimalModel
            );

            // üéØ STEP 8: Registrar gasto real
            if (userId && actualCost > 0) {
                await aiCostPredictor.recordSpending(userId, actualCost, optimalModel);
            }

            // üéØ STEP 9: Cachear el resultado para futuros usos
            const totalTokens = completion.usage?.total_tokens || 0;
            await AnalysisCacheService.cacheAnalysis(
                requirement, 
                content, 
                totalTokens, 
                optimalModel
            );

            // üéØ STEP 10: Registrar m√©tricas y costos
            if (completion.usage && userId) {
                await AIMetricsService.recordUsage({
                    userId,
                    requestType: 'analysis',
                    model: optimalModel,
                    promptTokens: completion.usage.prompt_tokens,
                    completionTokens: completion.usage.completion_tokens,
                    totalTokens: completion.usage.total_tokens
                });

                await TokenCostControlMiddleware.recordTokenUsage(
                    userId,
                    optimalModel,
                    completion.usage.prompt_tokens,
                    completion.usage.completion_tokens
                );
            }

            const savings = costEstimate.estimatedCost - actualCost;
            StructuredLogger.ai('Analysis generated with cost tracking', { 
                method: 'analyzeRequirement', 
                totalTokens,
                estimatedCost: costEstimate.estimatedCost,
                actualCost,
                savings
            });

            return {
                success: true,
                data: content,
                usage: completion.usage ? {
                    promptTokens: completion.usage.prompt_tokens,
                    completionTokens: completion.usage.completion_tokens,
                    totalTokens: completion.usage.total_tokens
                } : undefined,
                costInfo: {
                    estimatedCost: costEstimate.estimatedCost,
                    actualCost,
                    model: optimalModel,
                    savings: savings > 0 ? savings : undefined
                },
                circuitBreakerUsed: true
            };
        } catch (error) {
            // Handle Circuit Breaker specific errors
            if (error instanceof CircuitBreakerOpenError) {
                StructuredLogger.warn('Circuit breaker prevented AI request', { 
                    method: 'analyzeRequirement',
                    error: error.message
                });
                return {
                    success: false,
                    error: 'AI service temporarily unavailable. Please try again later.',
                    circuitBreakerUsed: true
                };
            }

            StructuredLogger.error('OpenAI Analyze Requirement Error', error as Error, { method: 'analyzeRequirement' } as any);
            return {
                success: false,
                error: error instanceof Error ? error.message : 'Failed to analyze requirement'
            };
        }
    }

    async generateQuestionsFromAnalysis(analysis: string): Promise<AIResponse> {
        try {
            // üéØ STEP 1: Verificar cach√© de preguntas primero
            const cachedQuestions = await AnalysisCacheService.getCachedQuestions(analysis);
            if (cachedQuestions) {
                StructuredLogger.info('Using cached questions', { method: 'generateQuestionsFromAnalysis' });
                return {
                    success: true,
                    data: cachedQuestions.join('\n'),
                    usage: {
                        promptTokens: 0,
                        completionTokens: 0,
                        totalTokens: 0
                    }
                };
            }

            // üéØ STEP 2: Optimizar para m√°ximo ahorro
            const optimizedPrompt = TokenOptimizer.optimizePrompt(PROMPTS.QUESTIONS);
            const optimizedAnalysis = TokenOptimizer.truncateContext(analysis, 800); // M√°s agresivo
            const optimalModel = 'gpt-3.5-turbo'; // Forzar modelo m√°s barato
            const taskConfig = this.optimizeConfigForTask('questions');

            const completion = await openai.chat.completions.create({
                model: optimalModel,
                messages: [
                    { role: "system", content: optimizedPrompt },
                    { role: "user", content: optimizedAnalysis }
                ],
                temperature: taskConfig.temperature,
                max_tokens: Math.min(taskConfig.maxTokens || 1000, 600), // L√≠mite m√°s estricto
            });

            const content = completion.choices[0]?.message?.content;
            if (!content) {
                throw new Error('No response from OpenAI');
            }

            // üéØ STEP 3: Cachear las preguntas generadas
            const totalTokens = completion.usage?.total_tokens || 0;
            const questions = content.split('\n').filter(q => q.trim().length > 0);
            
            await AnalysisCacheService.cacheQuestions(
                analysis,
                questions,
                totalTokens,
                optimalModel
            );

            StructuredLogger.ai('Questions generated', { method: 'generateQuestionsFromAnalysis', totalTokens });

            return {
                success: true,
                data: content,
                usage: completion.usage ? {
                    promptTokens: completion.usage.prompt_tokens,
                    completionTokens: completion.usage.completion_tokens,
                    totalTokens: completion.usage.total_tokens
                } : undefined
            };
        } catch (error) {
            StructuredLogger.error('OpenAI Generate Questions Error', error as Error, { method: 'generateQuestionsFromAnalysis' } as any);
            return {
                success: false,
                error: error instanceof Error ? error.message : 'Failed to generate questions'
            };
        }
    }

    async generateTestStrategySimple(analysis: string): Promise<AIResponse> {
        try {
            const completion = await openai.chat.completions.create({
                model: config.openai.model,
                messages: [
                    { role: "system", content: PROMPTS.STRATEGY },
                    { role: "user", content: analysis }
                ],
                temperature: 0.2,
                max_tokens: config.openai.maxTokens,
            });

            const content = completion.choices[0]?.message?.content;
            if (!content) {
                throw new Error('No response from OpenAI');
            }

            return {
                success: true,
                data: content,
                usage: completion.usage ? {
                    promptTokens: completion.usage.prompt_tokens,
                    completionTokens: completion.usage.completion_tokens,
                    totalTokens: completion.usage.total_tokens
                } : undefined
            };
        } catch (error) {
            StructuredLogger.error('OpenAI Generate Strategy Error', error as Error, { method: 'generateTestStrategySimple' } as any);
            return {
                success: false,
                error: error instanceof Error ? error.message : 'Failed to generate test strategy'
            };
        }
    }

    async generateQuestionsList(summary: string): Promise<string[]> {
        try {
            const prompt = `${PROMPTS.GENERATE_QUESTIONS}
"""${summary}"""`;

            const completion = await openai.chat.completions.create({
                model: config.openai.model,
                messages: [
                    { role: 'system', content: 'Eres un experto QA en an√°lisis de requisitos.' },
                    { role: 'user', content: prompt }
                ],
                temperature: 0.4,
                max_tokens: config.openai.maxTokens,
            });

            const result = completion.choices[0]?.message?.content;
            if (!result) {
                throw new Error('No se generaron preguntas.');
            }

            // Limpiar el bloque de markdown si viene en formato ```json\n...\n```
            const cleanResult = result
                .replace(/```json\n?/g, '')  // Quitar inicio de bloque markdown
                .replace(/```/g, '')         // Quitar cierre de bloque markdown
                .trim();

            const questions: string[] = JSON.parse(cleanResult);
            return questions;

        } catch (error) {
            StructuredLogger.error('OpenAI Generate Questions List Error', error as Error, { method: 'generateQuestionsList' } as any);
            throw new Error('Failed to generate questions list');
        }
    }
}

// Exportar una instancia √∫nica del servicio
export const openAIService = new OpenAIService(); 